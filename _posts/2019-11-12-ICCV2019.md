---

title: My Experiences at ICCV 2019

published: True

---

  

<style>

* {

box-sizing: border-box;

}

  

.column {

float: left;

width: 33.33%;

padding: 5px;

}

  

/* Clearfix (clear floats) */

.row::after {

content: "";

clear: both;

display: table;

}

</style>

With the fast-paced advancements in the machine learning community, conferences have become frontier of the field. Unlike many other science disciplines, most of the research in machine learning is presented in these conferences. Therefore, attending these conferences has become a very valuable activity. In fact, attending ML conferences is so valuable that it is hard to get tickets if you don't book it on time. For instance, [NIPS18 sold-out in 18]([https://medium.com/syncedreview/nips-tickets-sell-out-in-less-than-12-minutes-e3aab37ab36a](https://medium.com/syncedreview/nips-tickets-sell-out-in-less-than-12-minutes-e3aab37ab36a)) and almost every other conference of the field is witnessing huge growth in both the number of submitted papers and attendees [36, 37]. International Conference on Computer Vision (ICCV) is one such prestigious venue for both deep learning and computer vision. Fortunately, [ICCV19](http://iccv2019.thecvf.com/) was held in Seoul last week. It was a great opportunity since my university is near Seoul. And Thanks to my supervisor, [Dr. Sung-Ho Bae](https://sites.google.com/site/sunghobaecv/), I was able to attend it. I had an amazing seven days experience of attending workshops, tutorials, oral and poster presentations, intermingling with great researchers, hearing and understanding great ideas of the field and much much more.

  

  

![enter image description here](https://lh3.googleusercontent.com/HCCrMOiaoJYluGJAPLUCmqytmUsUjILqs_e13KXd6QXwWbfYViOs8npGFRbObBnXNcNB35aP6PI)

  

Here, I will try to write about my first experience of attending a science conference.

  

  

### Workshops and Tutorials

  

On the first day, I attended two different talks from [Workshop on statistical deep learning]([http://www.sdlcv-workshop.com/](http://www.sdlcv-workshop.com/)) and [robust subspace leanring]([https://rsl-cv.univ-lr.fr/2019/](https://rsl-cv.univ-lr.fr/2019/)). The first [talk](http://www.sdlcv-workshop.com/slides/talk_WuKorea1.pdf) was on the matrix and vector representation in Neuroscience by [Prof. Ying Nian Wu](http://www.stat.ucla.edu/~ywu/).

The main theme of the talk was around the place and grid cells in the brain of mammalian that are responsible for navigation [1]. The place cells fire only when we encounter a specific place while the grid makes certain hexagonal firing patterns. These neurons are responsible for path planning, motion and even called our GPS because of their assistance in navigation even in the absence of visual clues. This paper [1] showed that the self-location of an agent can be represented by a vector and self-displacement with a matrix in higher dimensions in such a way that path planning, displacement, and many other activities can be represented as transformations on these cells. They even showed that hexagonal patterns emerged by these cells can also be recovered via these higher dimensional representations.

The second talk was on Tensor factorization for robust learning by Jean Kossaifi which was based on [2]. The talk was around the theme of the use of tensor decomposition in deep neural networks which decreases computation complexity and increases robustness. The showed representing whole neural network with a higher dimensional tensor makes it lightweight, more robust while maintaining accuracy. Jean Kossaifi is also the author of a famous python library for this purpose called [Tensorly](http://tensorly.org/)

  

In the second half of this day, I attended the tutorial on [interpretation of Deep learning](https://interpretablevision.github.io/) which was jam-packed similar to its predecessor in CVPR19.

  

![](https://interpretablevision.github.io/figures/iccv19_meeting.jpg)

  

  

The first talk of this tutorial was on [Understanding models via visualizations and attribution](https://interpretablevision.github.io/slide/iccv19_vedaldi_slide.pdf) by [Andrea Veladi]([http://www.robots.ox.ac.uk/~vedaldi/](http://www.robots.ox.ac.uk/~vedaldi/)). We know that despite the impressive performance, deep neural networks are black boxes and understanding is very important. We can divide understanding them into three parts; what does a network do, how does it do it and how does it learn. This talk was on the second aspect i.e. how does a network do a specific task. And this talk was revolving around two central questions; what does an intermediate layer of DNN know about the input and what does a neural network see in the input image.

*First Question*: What a model knows of input image $x$ at an intermediate layer $y$, as shown in the following figure.

![](https://lh3.googleusercontent.com/E1OAdYaxnHR9VSJE22DQ_Hrcn0zDZaljyMgSzQy8R4tefreBd6cooJU4YQFVr4DZM72FZlx7m-0)

  

One way to answer this question is to project $y$ on input space and get $x_0$. This can be done by simple reconstruction i.e. start from a random initialization of $x_0$ and minizine $$||\Phi(x) - \Phi(x_0)||$$ and get so-called pre-image or $x_0$. This reconstruction is done without any training data. There are several ways to reconstruct this pre-image such as presented by [3,4,5]. Interestingly, reconstruction from any layer shows a striking similarity to its input. Even the last fully connected layer, despite having only information of output probabilities, do show similarity with the input image.

  

![enter image description here](https://lh3.googleusercontent.com/HdTi6nI7izhpCNIGuVPHJ0UDVetch85RkAFxLY25WSZG1nRktAfD1hZERQme0ufDhn8u9bqFiPs)

  

The deep image prior [4] has previously shown a very interesting result that we can fit a neural network with a single image and can perform many tasks. If we fit a neural network with a single image and then ask the above-mentioned question from this neural network, it still gives a very clear reconstruction of the input image. This is interesting because the neural network is not trained on any data. For an interesting discussion on this topic, see [this](https://distill.pub/2018/building-blocks/)

*Question 2* What do a neural network see in the input image. One straight forward way to answer this question is by visualizing gradients of a network w.r.t to its input image [6,7,8].

  

![enter image description here](https://lh3.googleusercontent.com/LVVJMwmgJnmkFjsyoqODc0EzRkdMORCKH3xQ_HXjO0kc-i3xvPRcHstapyCF-NDv1OiSZqySrso)

  

These visualizations do give some hints but they are not very clear. This problem can be solved by smoothing gradients [9]. But, all of these visualizations methods have a problem of channel specificity i.e. changing output class does not change visualization.

![](https://lh3.googleusercontent.com/Vjfu-y96okYLdlUTAGTZNEbxA7RX00KpdAald1HOdVE1jegnpRecd7DUO3rKm9qoTZts82SXyII)

  

This problem can be solved with CAM and GradCAM [9,10] where we use an intermediate layer to see prominent parts of the input image.

One question that is worth asking is what do these visualizations mean? Andrew argued that gradient can be thought of as a local linear approximation of the model so we can think of gradient-based visualizations as a sensitivity analysis. This is where new work on perturbation analysis [12] from him and his co-authors comes along which is to understand neural network via interesting perturbations i.e. by injecting noise, rotation, erasing parts of the input, etc. and see network behavior.

  

![](https://lh3.googleusercontent.com/xwGC_5cqRysKmcU1p6odc0cxizzh58T9RakYICITzbJZQmOOwVUTnE-TIScMdbTjb01a5T1_DqI)

  

This analysis gives several interesting results such as foreground might be sufficient but eliminating background can negatively affect the network, and many more.

  

The second talk was delivered by [Bolei Zhou](http://bzhou.ie.cuhk.edu.hk/) on understanding latent semantics of GAN. In the last some years, GANs have become better at producing high quality, realistic and diverse images. This talk was on understanding this generation process. The talk revolved around three things; the understanding role of neurons, semantics in latent space and biases and limitations.

  

The first question was on the role of individual neurons in image generation. GAN dissection paper [13] solved this riddle by turning on-off individual neurons and then using semantic segmentation to find the effect of it on generated images. Interestingly, they found that different neurons are responsible for different kinds of objects and scenes in a GAN. For instance, one can find a neuron that is responsible to generate trees in a scene and then by turning this neuron off or on, one can include/exclude these objects. The following figure shows the effect of different units. You can try a demo of it [here](ganpaint.io).

  

![](https://lh3.googleusercontent.com/Q4kHb4eT0nEajX3t9l7xiGHbfzy57sLZMXkLUoTTkMnEqqEbZ8cSJiFBP0-crDD2XEzjssBx14Y)

  

GANs generate image given a random vector from latent space. A random walk in this latent space changes the generated image in a meaningful way. This shows the relationship between this vector and the semantics of the generated image. In ongoing work, Bolei Zhou et all. showed that it is, in fact, possible to classify latent space according to attributes. For example, one can find the boundary of latent space to turn indoor lights on/off.

  

The third question was, can we use GAN to reconstruct an image i.e. given an image $x$, can we find its latent space $z$ such that GAN can reproduce original image $x$. While GAN can reconstruct images, the generated image turns out to be different than original as shown in the figure.

  

![enter image description here](https://lh3.googleusercontent.com/uOLADqFIBAWAvXTEXYCdFNq-AH0bvmXOvcBZOzGS28eVrrmPDYdAI6OKxg-tnmY0wMxKBqBBazs)

  

As shown in paper [14], GAN doesn't like generating many objects such as people, vehicles, signs, etc. Similarly, given an Asian face, GANs tend to make it white. This shows the limitations and biases of the GAN?

  

After the talk, I was able to ask a few questions from Zhou. One question that came to my mind from Asian face to the Whiteface phenomenon is what is the root of issues in reconstruction; data or GAN? To me, it seems like a data rather than a GAN problem.

  

The third and last talk of the session and the day [Explaining deep learning for identifying structures and biases in computer vision](https://interpretablevision.github.io/slide/iccv19_binder_slide.pdf) by [Prof. Alexander Binder](https://scholar.google.de/citations?user=5B8CTlEAAAAJ&hl=de). The talk was around the concept of layer-wise relevance propagation (LRP) to interpret neural networks. In this method, the score for each dimension of input is calculated and used as an explanation. This method successfully found game strategies learn by DNN to play atari breakout. He also discussed different ways to find biases in datasets.

  

On the second day of the workshops and tutorials, I decided to take [Neural Architects](https://neuralarchitects.org/) workshop arranged by the famous VGG group of Oxford University. The first talk was [Neural Architecture and Beyond](https://neuralarchitects.org/slides/zoph-slides.pdf) by [Barret Zoph](http://barretzoph.github.io/). This talk gave a holistic view of progress in the architectures of neural networks and NAS. They divided AI into four periods; good old fashioned where handcrafted features were used without any learning, second when handcrafted features were used with a learning algorithm, third recent deep learning wave where most of the feature engineering is done at the algorithmic level i.e. change of architecture, etc. and fourth learn to learn or neural architecture search, a recent phenomenon.

![](https://lh3.googleusercontent.com/iH-WJ-K9LFmdx9un13pXTigU_c-5iHifEsFOHvrSq1pwFUAYEym2TDjwqLrUT7Y9cQ8woxrWUBE)

  

While it can be argued that most of the recent benefits in Deep Learning came from improvements in architecture (AlexNet [16], VGG [17], ResNet[18], DenseNet[19], SENet [20], etc. ), this is also one of the most difficult, time consuming and almost more of an artistic kind of a job. It requires a lot of human effort, intuition, and experimentation. Therefore, this is an ideal task for machine learning i.e. make a neural network to learn the architecture of the neural network. In the modern deep learning era, this was initiated by [21, 22]. Specifically, these papers introduced a reinforcement learning way to find the best architectures. In this setting, a controller proposes an architecture from a search space, this proposed network is trained on a subset of the dataset, then this trained classifier's accuracy and other relevant parameters are fed to the controller as a reward. This way, the controller learned to output the best performing architecture.

![](https://lh3.googleusercontent.com/Ys4Dwm8OCUmntwFLt6pjjXTN484ckM939msn8l4HDEISShUXqpHxFP-R2YqdOp2k-SqES5qe2mk)

  

Mostly, an RNN controller is used while search space consists of different architectural elements such as convolution layer, BN layers, connections, etc. Using this way, efficient architecture can be found in almost any field. It has been shown to work in classification, segmentation, video processing, machine translation, and many other tasks as well.

It is also interesting to inspect decisions made by these controllers. While most of the decisions are not very understandable, a few do give interesting insights. For instance, in many of the proposed architectures, more convolution layers are used at the start than at the end.

In recent times, many variants of NAS has been introduced. Platform aware NAS [23] is used to find an architecture that is both better in accuracy and latency. To achieve this, latency and accuracy based reward is defined. EffecientNAS [24] solves the challenge of high computations required by NAS by assuming search space as a big graph where each step has multiple options and NAS's job is to find the better path. The neural network selected by the controller is trained for a few steps and weights are shared from previous training.

  

![Effecient NAS](https://lh3.googleusercontent.com/7naqN1DdEGBdmD-UnRIbalFgtUr_leIR-liLZGF7aRt14vDIQ90Ry4IMFmSEQH6GHEKWHbLzeCOW)

  

Similarly, NAS has also been used to find the best data augmentation policy [25]. In this, we try to find three parameters; which operation to perform (rescaling, rotation, etc.), with what probability and magnitude. In [26], the authors found that by limiting this search space to find only probability and magnitude also work well.

**[My question](https://youtu.be/O5Rrv6BvdgE?t=1724) for this talk**: Human devised architectures are very generalizable i.e. an architecture made for classification(Alexnet, Resnet) also works well for many many other tasks. Is it the same for NAS as well? And can we define some notion of overfitting in NAS i.e. if an architecture works well on a task it was found by but fail to work on different tasks? You can hear the question and answer [here](https://youtu.be/O5Rrv6BvdgE?t=1724)

  

The second talk was [Towards General Vision Architectures: Attentive Single-Tasking of Multiple Task]([https://neuralarchitects.org/slides/kokkinos-slides.pdf](https://neuralarchitects.org/slides/kokkinos-slides.pdf)) by [Iasonas Kokkinos](http://www0.cs.ucl.ac.uk/staff/I.Kokkinos/). This talk was about finding a multi-tasking network. To motivate one-network-multiple-tasks, one can see how many things we can extract from one image.

  

![](https://lh3.googleusercontent.com/-sr9IeRlzern8rK37tU1vBpJJYX638rp6rH8OnHn6HY8jOQMe2141NDKrvScThJop0ExXJc-SzU1)

We get a performance boost if we use one network for a few tasks but as the number of tasks increases, the performance actually decreases. One way to solve this problem is to increase the task-specific processing of the network. But this makes multi-tasking less lucrative as gain in terms of required computations diminishes.

  

We know that multi-tasking does work for some tasks. But to get a full view, we need to get a few things straight. First, some tasks cannot be done via the same network due to task inference [27, 28].

  

![](https://lh3.googleusercontent.com/rhVClpewevhbeDdiajChcC3yExgbq4UhCyXgYIb-_oiQ-6J4nxfyJzUHP22y99QGl78o5N_5BN-9)

  

Similarly, it is even difficult for humans to do multi-tasking. To demonstarte this point, one interesting [example](https://www.youtube.com/embed/vJG698U2Mvo) was shown. The solution to this problem, according to him, is to give each task space i.e. divide the network into shared and task-specific space.

![](https://lh3.googleusercontent.com/xEP-MVCnvaDvQX9GzyLoG_AR9qkaQWcyX4yGPw6y3vchpuexhJc8qRg78JU6CfNPd8Vkt2RT25uH)

  

One way to do this to first define a general network and then, for each task, find relevant blocks and their connections [29, 30, 31, 32]. The following picture shows one such setting.

![](https://lh3.googleusercontent.com/3JZkO4Fxooj9ZBD1-YZdDLSLKMAltOfOiEf-6jvdMiLcQ1OXrfa5-hUXeWmpITf9NYpE-75YA_O8)

![](https://lh3.googleusercontent.com/VJuhTyUmac5MSLV6CbtaAV8O_uxvx65Ow38CA7jV421cK1qAp2zhQ5cKAZoB8BW8ekfIJ9U5jE-c)

  

But these strategies have a combinatorial search problem. Here comes the work done by presenters called attentive single-tasking of multiple tasking[33]. The main aim of the work is to make a network to perform one task at a time instead of multiple in the same forward pass, encourage relevant features and ignore irrelevant features. To do this, they start with an encoder-decoder setup. They made encoder task-specific and make decoder learn universal representation. To make encoder task-specific, the same backbone is used with task-specific attention mechanisms [34] to focus on task-specific features while ignoring task-irrelevant features, task-specific residual adapters [35] and on top of the network, an adversarial task discriminator. These additional layers keep 95% of the network shared while also create task-specific space.

![](https://lh3.googleusercontent.com/-wLSA6cpHR10mauif1c9CDcFMFHo6E9fkUWRL7tgjnSaSETL85gL0h6z_Zkd3a-T0M30FZW35EnD)

  

On the last day of the conference, I had presented our paper in the workshop on computational imaging. This paper was a collaboration work with my ex. colleagues form [Spider Lab](http://spider.itu.edu.pk/).

![enter image description here](https://lh3.googleusercontent.com/KfM87jdALof7WTcoJ8mFsI8ajD_AVDNo0YAgCq8QOUaVOcpM7_iYBbkeUgiOo2dQ38Ucd265frP9)

  

  

### Interesting Papers

- Why Does Data-Driven Beat Theory-Driven Computer Vision?
- GeoStyle: Discovering Fashion Trends and Events
- Symmetric Cross-Entropy for Robust Learning with Noisy Labels
- NLNL: Negative Learning for Noisy Labels
- Adversarial Robustness vs. Model Compression, or Both?
- What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance
- Exploring Randomly Wired Neural Networks for Image Recognition
- Exploring Randomly Wired Neural Networks for Image Recognition
- Transferability and Hardness of Supervised Classification Tasks
- Switchable Whitening for Deep Representation Learning
- Image Generation From Small Datasets via Batch Statistics Adaptation
- Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks
- EvalNorm: Estimating Batch Normalization Statistics for Evaluation
- Seeing What a GAN Cannot Generate
- Sparse and Imperceivable Adversarial Attacks
- Improving Adversarial Robustness via Guided Complement Entropy
- CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features
- No Fear of the Dark: Image Retrieval Under Varying Illumination Conditions

### Socializing with other Researchers

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Best advice on attending conferences I&#39;ve received.<br><br>Papers &amp; talks are online. The main value is the people. Look up authors/speakers/Twitter in advance to see who&#39;s going and email them asking for a quick chat. Talk to everyone, especially those whose work isn&#39;t already famous.</p>&mdash; Chip Huyen (@chipro) <a href="https://twitter.com/chipro/status/1196889061799055360?ref_src=twsrc%5Etfw">November 19, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

One of the main attractions of ICCV for me was a chance to meet bigwigs of our community. I use this opportunity to its fullest. Apart from arranging a meetup for Pakistanis attending ICCV, I also had lengthy discussions with many researchers.

- [Prof. Ping Luo](http://luoping.me/) : I was following [Prof. Ping Luo](http://luoping.me/) (Hong Kong University, ex Research director at SenseTime) due to his work on Batch Normalization. Upon my request, he agreed to meet me during the conference to discuss possible internship opportunities with him. It was really great talking with him about his research.

- [Prof Mubark Shah](https://www.crcv.ucf.edu/person/mubarak-shah/): Dr. Shah is a very famous professor of computer vision. Since he has been working in this field for very long and know conventional cv and has seen neural networks rising up so it one of the major topics of discussion with him was his views on this change. We also discussed about Pakistan since he is a fellow Pakistani.

- [Porf. Nasir Rajpoot](https://warwick.ac.uk/fac/sci/dcs/people/Nasir_Rajpoot/): He is leading Tissue Image Analytics Labs at the University of Warwick. Since he has also witnessed great changes in the field so a major topic of discussion was conventional machine learning in medical versus deep learning.

- [Dr. Salman Khan](https://salman-h-khan.github.io/): Dr. Salman is a very active computer vision researcher, a lecturer at Australian National University (ANU) and Senior Scientist at [Inception Institute of AI](http://www.inceptioniai.org/). I asked him to spare some time and he was kind enough to invite me for lunch. There we had a discussion for 2-3 hours. This was probably one of the best discussions I ever had on artificial intelligence, computer vision, and machine learning. I was impressed with his knowledge of the practical side of deep learning. He gave me really good tips to sharpen-up my doing-experiments-quickly skills. We discussed many topics ranging from research, Pakistan, AI community and many more. Amazingly, I was able to learn so many things from him even in this short discussion. So, special thanks to him for sparing time for young researchers.

  

- [Dr. Arslan Basharat](https://www.kitware.com/arslan-basharat/): Dr. Basharat is an amazing person to have a discussion with. One of the major topics of discussion with him was hardships in the life of a graduate student and how to overcome it. He also told us about his work on deep fake detection.

- [Dr. John K. Tsotsos](http://www.cse.yorku.ca/~tsotsos/Tsotsos/Home.html) on his views on deep learning and its limitations.

- [ Dr. Iasonas Kokkinos](http://www0.cs.ucl.ac.uk/staff/I.Kokkinos/) discussion on multi-tasking neural networks.

- [Dr. Ross Girshick](http://www.rossgirshick.info/) on many general topics like how research is conducted, how suddenly things came out straight from a mess, the stress of research process and his work on randomly wired networks.

- [Dr. Bolei Zhou](https://people.csail.mit.edu/bzhou/) on his talk about GANs and his generally his work on the interpretability of deep learning.

-[Dr. Seong Joon](https://seongjoonoh.com/): Discussed with him about his research on the interpretation of deep learning.

 
## Meetup of Pakistanis@ICCV

One of the exciting things at the conference was to see so many Pakistanis from around the globe attending it. While I met a few, it was not possible to meet everyone. Therefore, I presented the idea of arranging a meet up with some other young researchers. This was an amazing event attended by around 30 Pakistanis ranging from early-researchers to famous professors and researchers like [Prof Mubark Shah](https://www.crcv.ucf.edu/person/mubarak-shah/), Prof. [Porf. Nasir Rajpoot](https://warwick.ac.uk/fac/sci/dcs/people/Nasir_Rajpoot/), [Dr. Arslan Basharat](https://www.kitware.com/arslan-basharat/), [Dr. Salman Khan](https://salman-h-khan.github.io/), Dr. Basharat and many more. It was an amazing feeling to be able to meet so many accomplished Pakistanis. We end up doing two meetups to accommodate people that were unable to attend the first one.

![](https://lh3.googleusercontent.com/UWatJm9p_eg8pD-Vryc-jaUO7fdqTjvteC9f7_e2h4ZDn3zwepfTMccPDlXKzYANSFY3YO5BB-WY)
 ![](https://lh3.googleusercontent.com/C-5_c_xDIHvfNT1XrC2vG5gf1cPWCyMdbK_6zaY8yaWStV0vzlRPghz1zsYLvU4pYZ97zr21djpB)
 ![](https://lh3.googleusercontent.com/yuZGIiTM7pEP-xBI7MHzDo63Ln9CBjkRFHO0apOhTJmIN2hRPpVdMuwJpPBye_CH0YLgftC3M8SI)
  

## Other moments

|![enter image description here](https://lh3.googleusercontent.com/Qji-rloPLmfxX270-IGBU0j4aOglWNKqSM4uJFBuRsXyruABA56i_TLgSLXT7M7NKOG74SfS7JuQ) | ![enter image description here](https://lh3.googleusercontent.com/OzQBY5s-0yqMb3k1IDOrKVq-5gjR64CxKqNtOVjPbphLfmSm-zwdIsM67HZBzcKTqB9m-jvHEFjd) 
| ![enter image description here](https://lh3.googleusercontent.com/Sbjbdc1ljpT-mupOTL2HaMB3Vf3euP8HEAseRg1GBGQ8_dItOwg--in8NS7N0mDI1faMMgS0UwNx) | ![enter image description here](https://lh3.googleusercontent.com/L9cF6RtpwMbvABAw-uxbtT92ezh6kJ_Mjc1COhNCU0R2Y91dmV3f_bYVCvKji_hzAXI7ICfyD9Si) |

|![enter image description here](https://lh3.googleusercontent.com/Hd37onblLq3o1nFh5yIBqiIonVKbONpnNvBOFk7oo9nlRcIiuGMmU9upvHLWJxyZPLe_6Jhocho7) | ![enter image description here](https://lh3.googleusercontent.com/Zdi_b2TNhbxnmpl_tmQeY9w8V3Q3TrWUp3OGZ-iQOZpWCwRlh6V8XvAdGA6ur-_SZbRhfdtIWVq3) |

|![enter image description here](https://lh3.googleusercontent.com/xINJJMvJD2NqDa8geC9huuKI9j3E9FCHtMh-MWbXGbJfZzA8YJ2IW5_7FNrQsTvrGiN0-HphExkn) | ![enter image description here](https://lh3.googleusercontent.com/MEJlgTb7ie6Rk-ZWcoZYWEv4QkAEXVC1Ku7JxQGPm4ljszZO_ahoRys-y_vR4U7IFprLcQgHvXDY) |
![enter image description here](https://lh3.googleusercontent.com/AO8Zyu7yRc_SpzbgOyQ3Ij94cWjXRc1DlqsC0EZF9gXqDr2wguQPnEnhcYJK7FFeH2Vi-NwK_TnB)|![enter image description here](https://lh3.googleusercontent.com/KN6BGbnsrQDIOBARoPFC79-9_8G4XXmWPV9-LX5MCWIWAAJDx4EGXC-j0NDqpTBskdm7KbdEmgvT)|

  

  

  

  

  

### References

[1] Gao, Ruiqi, et al. "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion." (2018).

  

[2] Kossaifi, Jean, et al. "T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor." _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_. 2019.

  

[3] Mahendran, Aravindh, and Andrea Vedaldi. "Understanding deep image representations by inverting them." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2015.

 
[4] Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. "Deep image prior." _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_. 2018.

  

[5] Nguyen, Anh, et al. "Plug & play generative networks: Conditional iterative generation of images in latent space." _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_. 2017.

  

[6] Zeiler, Matthew D., and Rob Fergus. "Visualizing and understanding convolutional networks." _European conference on computer vision_. Springer, Cham, 2014.

  

[7] Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. "Deep inside convolutional networks: Visualising image classification models and saliency maps." _arXiv preprint arXiv:1312.6034_ (2013).

  

[8] Springenberg, Jost Tobias, et al. "Striving for simplicity: The all convolutional net." _arXiv preprint arXiv:1412.6806_ (2014).

  

[9] Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. "Axiomatic attribution for deep networks." _Proceedings of the 34th International Conference on Machine Learning-Volume 70_. JMLR. org, 2017.

  

[10] Zhou, Bolei, et al. "Learning deep features for discriminative localization." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2016.

  

[11] Selvaraju, Ramprasaath R., et al. "Grad-cam: Visual explanations from deep networks via gradient-based localization." _Proceedings of the IEEE International Conference on Computer Vision_. 2017.

  

[12] Fong, Ruth, Mandela Patrick, and Andrea Vedaldi. "Understanding Deep Networks via Extremal Perturbations and Smooth Masks." _Proceedings of the IEEE International Conference on Computer Vision_. 2019.

  

[13] Bau, David, et al. "Gan dissection: Visualizing and understanding generative adversarial networks." _arXiv preprint arXiv:1811.10597_ (2018).

  

[14] Bau, David, et al. "Seeing What a GAN Cannot Generate." _Proceedings of the IEEE International Conference on Computer Vision_. 2019.

  

[15] Montavon, Gr√©goire, et al. "Layer-wise relevance propagation: an overview." _Explainable AI: Interpreting, Explaining and Visualizing Deep Learning_. Springer, Cham, 2019. 193-209.

  

[16] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." _Advances in neural information processing systems_. 2012.

  

[17] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." _arXiv preprint arXiv:1409.1556_ (2014).

  

[18] He, Kaiming, et al. "Deep residual learning for image recognition." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2016.

  

[19] Huang, Gao, et al. "Densely connected convolutional networks." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2017.

  

[20] Hu, Jie, Li Shen, and Gang Sun. "Squeeze-and-excitation networks." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2018.

  

[21] Zoph, Barret, et al. "Learning transferable architectures for scalable image recognition." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2018.

  

[22] Real, Esteban, et al. "Large-scale evolution of image classifiers." _Proceedings of the 34th International Conference on Machine Learning-Volume 70_. JMLR. org, 2017.

  

[23] Wu, Bichen, et al. "Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search." _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_. 2019.

  

[24] Pham, Hieu, et al. "Efficient neural architecture search via parameter sharing." _arXiv preprint arXiv:1802.03268_ (2018).

  

[25] Cubuk, Ekin D., et al. "Autoaugment: Learning augmentation policies from data." _arXiv preprint arXiv:1805.09501_ (2018).

  

[26] Cubuk, Ekin D., et al. "RandAugment: Practical data augmentation with no separate search." _arXiv preprint arXiv:1909.13719_ (2019).

  

[27] Kang, Zhuoliang, Kristen Grauman, and Fei Sha. "Learning with Whom to Share in Multi-task Feature Learning." _ICML_. Vol. 2. No. 3. 2011.

  

[28] Romera-Paredes, Bernardino, et al. "Exploiting unrelated tasks in multi-task learning." _International conference on artificial intelligence and statistics_. 2012.

  

[29] Murdock, Calvin, et al. "Blockout: Dynamic model selection for hierarchical deep networks." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2016.

  

[30] Ahmed, Karim, and Lorenzo Torresani. "Maskconnect: Connectivity learning by gradient descent." _Proceedings of the European Conference on Computer Vision (ECCV)_. 2018.

  

[31] Veniat, Tom, and Ludovic Denoyer. "Learning time/memory-efficient deep architectures with budgeted super networks." _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_. 2018.

  

[32] Fernando, Chrisantha, et al. "Pathnet: Evolution channels gradient descent in super neural networks." _arXiv preprint arXiv:1701.08734_ (2017).

  

[33] Su, Jong-Chyi, Subhransu Maji, and Bharath Hariharan. "When Does Self-supervision Improve Few-shot Learning?." _arXiv preprint arXiv:1910.03560_ (2019).

  

[34] Hu, Jie, Li Shen, and Gang Sun. "Squeeze-and-excitation networks." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2018.

  

[35] Rebuffi, Sylvestre-Alvise, Hakan Bilen, and Andrea Vedaldi. "Learning multiple visual domains with residual adapters." _Advances in Neural Information Processing Systems_. 2017.

  

[36] [https://medium.com/syncedreview/aaai-2020-reports-record-high-paper-submissions-67f88080c93c](https://medium.com/syncedreview/aaai-2020-reports-record-high-paper-submissions-67f88080c93c)

  

[37] [https://medium.com/syncedreview/cvpr-2019-accepts-record-1300-papers-91b9e3b315f5](https://medium.com/syncedreview/cvpr-2019-accepts-record-1300-papers-91b9e3b315f5)
